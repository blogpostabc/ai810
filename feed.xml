<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://blogpostabc.github.io/ai810/feed.xml" rel="self" type="application/atom+xml"/><link href="https://blogpostabc.github.io/ai810/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-02T12:03:26+09:00</updated><id>https://blogpostabc.github.io/ai810/feed.xml</id><title type="html">AI810 Blog Post (20238037)</title><subtitle>Home to AI810 Blog Post (20238037) </subtitle><entry><title type="html">Analysing The Spectral Biases in Generative Models</title><link href="https://blogpostabc.github.io/ai810/blog/analysing-the-spectral-biases-in-generative-models/" rel="alternate" type="text/html" title="Analysing The Spectral Biases in Generative Models"/><published>2025-04-28T00:00:00+09:00</published><updated>2025-04-28T00:00:00+09:00</updated><id>https://blogpostabc.github.io/ai810/blog/analysing-the-spectral-biases-in-generative-models</id><content type="html" xml:base="https://blogpostabc.github.io/ai810/blog/analysing-the-spectral-biases-in-generative-models/"><![CDATA[<h1 id="viewing-images-in-frequency-domain">Viewing Images in Frequency Domain</h1> <p>While we typically view images in the spatial domain—where every pixel directly represents brightness or color—another compelling perspective is to examine them in the frequency domain. By applying the 2D Discrete Fourier Transform (DFT)<d-footnote>checkout <a href="https://github.com/Inspiaaa/2D-DFT-Visualisation" target="_blank">this</a> nice demo</d-footnote> <d-cite key="schwarz2021frequencybiasgenerativemodels"> </d-cite>, we can break down an image into its frequency components, revealing hidden structures and patterns that aren’t as apparent in the spatial view. A 2D discrete fourier transform maps a grayscale image $I \in \mathbb{R}^{H \times W}$ to the frequency domain as follows :</p> \[\hat{I}[k, l] = \frac{1}{HW} \sum_{x=0}^{H-1} \sum_{y=0}^{W-1} e^{-2\pi i \frac{x \cdot k}{H}} \cdot e^{-2\pi i \frac{y \cdot l}{W}} \cdot I[x, y]\] <p>Here, $k=0,1,2…H-1$ and $l=0,1,2,…W-1$. So it outputs an image in the frequency domain of size ${H \times W}$. Here $\hat{I}[k, l]$ is a complex value at the pixel $I[x, y]$. For example, given below is a grayscale image of a baboon<d-footnote>Image taken from <d-cite key="schwarz2021frequencybiasgenerativemodels"></d-cite>.</d-footnote> viewed in the frequency domain.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/baboon_spectrum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/baboon_spectrum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/baboon_spectrum-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/baboon_spectrum.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>To enhance power spectrum visualization, we remove the DC component, apply a Hann window<d-footnote>refer to this <a href="https://en.wikipedia.org/wiki/Hann_function?utm_source=chatgpt.com" target="_blank">article</a></d-footnote> to reduce spectral leakage, normalize by the maximum power, and use a logarithmic scale. This ensures the most powerful frequency is set to 0.<d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite></p> <p>Frequency basically refers to the rate of change of colour in an imgae. The smooth regions (where colour or pixel intensities don’t change much) in an image correspond to low frequency while the regions containing edges and granular features (where colour changes rapidly) like hair, wrinkles etc correspond to high frequency. We can understand this by relating it to fitting 1D step functions using fourier series. The region of the step has large coefficients for the higher frequency waves while the other region has large coefficients for the low frequency waves.</p> <p>To analyse the frequency content, we estimate the Power Spectral Density (PSD) by squaring the magnitudes of the Fourier components. To visualise the spectrum as a 1D plot, the reduced spectrum S i.e. the azimuthal average over the spectrum in normalized polar coordinates $r \in [0, 1]$, $\theta \in [0, 2\pi)$ is calculated as<d-cite key="schwarz2021frequencybiasgenerativemodels"></d-cite> :</p> \[\tilde{S}(r) = \frac{1}{2\pi} \int_{0}^{2\pi} S(r, \theta) \, d\theta \quad \text{with} \quad r = \sqrt{\frac{k^2 + l^2}{\frac{1}{4}(H^2 + W^2)}} \quad \text{and} \quad \theta = \text{atan2}(k, l)\] <p>For the above image of baboon in frequency domain, the reduced spectrum is shown:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/reduced_spectrum-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/reduced_spectrum-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/reduced_spectrum-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/reduced_spectrum.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The power spectrum of natural images follows a power law i.e. $\frac{1}{f^\alpha}$ with $\alpha$ ~ 2. A more complete model of the mean power spectra (using polar coordinates) can be written as</p> \[E[|I(f, \theta)|^2] = \frac{A_s(\theta)}{f^{\alpha_s(\theta)}}\] <p>in which the shape of the spectra is a function of orientation. The function $A_s(\theta)$ is an amplitude scaling factor for each orientation and $\alpha_s(\theta)$ is the frequency exponent as a function of orientation. Both factors contribute to the shape of the power spectra<d-footnote>refer to <a href="https://web.mit.edu/torralba/www/ne3302.pdf" target="_blank">this PDF</a></d-footnote>. From here we can see that in natural images, the power spectrum is high in the low frequency region and low in the high frequency region. This is intuitive as we expect any natural image to have more smoother regions than edges and complex patterns.</p> <p>In digital imaging, understanding <strong>aliasing</strong> and the <strong>Nyquist</strong> Frequency is crucial for accurately capturing and representing details. When we convert a continuous scene, like a real world view into a digital image, we sample it at discrete intervals i.e. each pixel acts as a tiny snapshot of the scene at that point. The Nyquist Frequency, defined as half of the sampling rate, is the highest spatial frequency that can be accurately captured by this pixel grid. Let us see why. Imagine a 1D wave. To represent its frequency accurately, we need at least two samples per cycle: one to capture the crest (peak) and one to capture the trough. Since images are discretized in space, the maximum frequency is determined by the Nyquist frequency. For a square image, $H = W$, it is given by $f_{\text{nyq}} = \sqrt{k^2 + l^2} = \frac{H}{\sqrt{2}}$, i.e. for $r = 1$.</p> <p>If the details in the scene change more rapidly than this (i.e., they have a higher spatial frequency than the Nyquist limit), we will be unable to capture the shape of the actual wave and this high frequency oscillating region will be considered as a smooth low frequency region. Thus, when the pixel grid cannot capture both the crest and trough, or the rapid change of light intensity, this leads to a phenonmenon called aliasing. Aliasing causes high-frequency details to appear as misleading lower-frequency patterns or distortions, like the moiré effect<d-footnote>refer to <a href="https://en.wikipedia.org/wiki/Moir%C3%A9_pattern" target="_blank">this article</a></d-footnote> that often appears on finely patterned textures. Due to this, we capture the wrong spectrum where high frequency content is less and low frequency content is more than in the actual scene captured. This can be seen for a sinusoidal wave where we predict a wave with less frequency than the ground truth wave due to aliasing.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/aliasing-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/aliasing-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/aliasing-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/aliasing.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Image Source: <a href="https://www.youtube.com/watch?v=IZJQXlbm2dU" target="_blank">https://www.youtube.com/watch?v=IZJQXlbm2dU</a> </div> <h2 id="analysis-of-bias-in-gans">Analysis of Bias in GANs</h2> <p>Now, let us analyze spectral biases<d-cite key="rahaman2019spectralbiasneuralnetworks"></d-cite> in GANs<d-cite key="goodfellow2014generativeadversarialnetworks"></d-cite>. GANs have been quite successful in producing photo-realistic images. But things are a bit different when we view the produced images in the frequency domain. In this section we show that the ability of GANs to learn a distribution is significantly biased against high spatial frequencies i.e. GANs produce less high frequency content than in the actual image<d-cite key="lee2024spectrumtranslationrefinementimage"></d-cite>.</p> <p>This was earlier attributed to a mere scarcity of high frequencies in natural images, but recent works<d-cite key="chen2020ssdganmeasuringrealnessspatial"></d-cite><d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite><d-cite key="schwarz2021frequencybiasgenerativemodels"></d-cite> have shown that this is not the case. There are two main hypotheses that have been proposed for the spectral biases; one attributes it to the employment of upsampling operations<d-cite key="schwarz2021frequencybiasgenerativemodels"></d-cite>, and the other attributes it to linear dependencies in the convolution filter<d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite>, i.e., the size of the kernel deployed in the generator network. We take up these hypotheses in the remainder of this section.</p> <h3 id="setting-up-the-generative-cnn-structure">Setting Up the Generative CNN Structure</h3> <p>We’ll start by setting up the structure of a generative CNN model, which typically consists of a series of convolutional layers with filters that learn different features. Our CNN is structured as a stack of convolutional layers, with each layer represented as:</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/CNN_Image-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/CNN_Image-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/CNN_Image-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/CNN_Image.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> \[H_{l+1}^i = \text{Conv}_l^i(H_l) = \sum_c F_{l}^{i,c} * \text{Up}(\sigma(H_l^c))\] <p>where:</p> <ul> <li>$\text{H}_l$ : The feature map at layer <strong><em>l</em></strong>.</li> <li>$F_{l}^{i,c}$: A convolutional filter at layer <strong><em>l</em></strong>, of size ${k_l} \times \text{k}_l$ , that connects input channel <strong><em>c</em></strong> to output channel <strong><em>i</em></strong>.</li> <li>$\text{Up}(\cdot)$ : The upsampling operator, which increases the spatial dimensions, helping generate higher-resolution outputs.</li> <li>$\sigma(\cdot)$: A non-linearity function, typically a ReLU.</li> </ul> <p><strong>Inputs:</strong> The initial feature map is represented by $H_1$ with shape $d_0$ $\times$ $d_0$ .</p> <p><strong>Parameters:</strong> The model parameters are $W$ (weights for each layer).</p> <p><strong>Layers:</strong> The network is built from convolutional layers, each generating new feature maps based on its input. Each layer also performs upsampling and non-linear transformations to increase resolution and control spatial frequencies.</p> <p>Before starting with the analysis of a filter’s spectrum, we first need to introduce the idea of viewing ReLU as a fixed binary mask. Why do we need to do this? We’ll look at it in just a moment.</p> <h3 id="relu-as-a-fixed-binary-mask">ReLU as a Fixed Binary Mask</h3> <p>Considering ReLUs to be the activation $\sigma(\cdot)$, they can then be viewed as fixed binary masks<d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite> in the neighbourhood of the parameter $W$. Here, this means that for small variations in the parameters $W$, the activation pattern of the ReLU units (which inputs are passed and which are zeroed) does not change and since the ReLU outputs are determined by the sign of the pre-activation values, these signs only change at specific boundaries in the parameter space, thus ensuring binary mask remains fixed within any given region. We will now attempt to prove this.</p> <p>This proof has been inspired from the paper “Spatial Frequency Bias in Convolutional Generative Adversarial Networks”<d-cite key="khayatkhoei2020spatialfrequencybiasconvolutional"></d-cite> and focuses on showing that in a finite ReLU-CNN, the set of parameter configurations where the scalar output of the network crosses zero (i.e., changes sign) has a measure of zero. What is measure zero? A set of measure zero essentially means that the set occupies “negligible space” in the parameter space. In high-dimensional spaces like $\mathbb{R}^n$, measure-zero sets can often be thought of as lower-dimensional “slices” (e.g., lines, points, or surfaces) within the larger space. While they may exist mathematically, they are effectively insignificant in the context of the full parameter space.</p> <p>Mathematically, We are working with a scalar ouptut $\mathcal{f}(W)$ of a convolutional layer in a finite ReLU-CNN. Therefore, the function depends on the weight parameter <em>W</em> and the latent input $H_{1}$. Now, we need to show that for any neighbourhood around $W$, the output of the function is entirely non-negative or entirely non-positive. This means proving that the set of parameters where 𝑓 changes sign within every neighbourhood of 𝑊 (i.e., it crosses zero somewhere in every neighbourhood) has measure zero.</p> \[\implies G = \{ W \in \mathcal{W} \mid \forall \mathcal{N}(W), \exists U, V \in \mathcal{N}(W) : f(U) &lt; 0 &lt; f(V) \}\] <p>where $\mathcal{N}(W)$ represents the neighbourhood of $W$. $G$ captures the parameter values $W$ where $f(W)$ crosses zero in every neighborhood. Therefore, our objective becomes to show that $G$ has measure zero.</p> <p>A finite ReLU-CNN has a finite number of neurons and, hence, a finite number of ReLU activations. Each ReLU activation behaves like a piecewise linear function that “splits” the parameter space into regions. <br/> $\implies$ For any fixed configuration of active/inactive neurons, $f(W)$ becomes a polynomial function of $W$. Thus, for each configuration of ReLU activations, $f(W)$ behaves as a polynomial, with each configuration yielding a different polynomial form.</p> <p>A polynomial function on $\mathbb{R}^n \text{ to } \mathbb{R}$ has a measure zero set of zero-crossings in the parameter space <d-footnote> refer to <a href="https://www.researchgate.net/publication/281285245_The_Zero_Set_of_a_Polynomial" target="_blank">this ResearchGate article</a></d-footnote>. Intuitively, this means that the solutions to $f(W)=0$ occupy “negligible” space in the parameter space. <br/> $\implies$ A finite set of such polynomials also has a measure zero set of zero-crossings. $\therefore$ $G$ is also a measure zero set.</p> <p>Finally, this reasoning holds for any scalar output $f$ of the network, at any spatial location or layer. Given that there are only a finite number of such outputs in a finite network, the measure of $G$ for all outputs is still zero, thereby completing the proof.</p> <p>To summarize, the proof hinges on the fact that with ReLU activations, each layer’s output depends on whether each neuron is active or inactive. For any fixed set of active/inactive states, the network’s output behaves as a polynomial with respect to the parameters. Since polynomials only have zero-crossings on a measure zero subset of the parameter space, the overall network exhibits non-negative or non-positive output behavior <em>almost</em> everywhere in the parameter space.</p> <p>This implies that <em>almost</em> all regions of the parameter space are “stable” in terms of sign, and this stability is a result of the ReLU non-linearity creating a finite set of polynomial behaviors for 𝑓.</p> <h4 id="why-this-matters">Why This Matters</h4> <p>The key consequences and takeaways of this result are:</p> <p><strong>Simplified Frequency Control:</strong> Since the ReLUs act like fixed binary masks, they don’t introduce additional variability. The network’s spectral characteristics become easier to analyze because the ReLUs don’t actively change the frequency content in these neighbourhoods.</p> <p><strong>Shifts Control to Filters:</strong> The network’s ability to adjust the output spectrum depends more on the convolutional filters ${F}_l^{i,c}$ than on the non-linear ReLUs.</p> <h3 id="onto-the-analysis-of-filters">Onto The Analysis of Filters</h3> <p>Now that we have set up the base, we can now move on analyzing the effect of convolutional filters on the spectrum.</p> <p>The filters ${F}_l^{i,c}$ in each convolutional layer are the primary tools for shaping the output spectrum. Thus, the filters try to carve out the desired spectrum out of the input spectrum which is complicated by:</p> <ol> <li>Binary masks (ReLU) which although don’t create new frequencies, but distort what frequencies are passed onto the next layer.</li> <li>Aliasing from Upsampling.</li> </ol> <p>Now, take any two spatial frequency components \(U =\mathcal{F}_{l}^{i,c}(u_0, v_0)\) and \(V = \mathcal{F}_{l}^{i,c}(u_1, v_1)\) on the kernel $F_l$ of the $l$’th convolution layer of spatial dimension $d_l$ and filter size $k_l$, at any point during training. Let $G_l$ be a filter of dimension \(\mathbb{R}^{d_l \times d_l}\). Because of it’s dimension, it is an unrestricted filter that can hypothetically model any spectrum in the output space of the layer. Hence, we can write $F_l$ as a restriction of $G_l$ using a pulse P of area $k_l^2$ :</p> \[F_l = P.G_l\] \[P(x,y) = \begin{cases} 1, &amp; \text{if } 0 \leq x, y &lt; k_l \\ 0, &amp; \text{if } k_l \leq x,y \leq d_l \end{cases}\] <p>Applying Convolution Theorem on $F_l$:</p> \[\mathcal{F}_l = \mathcal{F}_{P} \cdot \mathcal{G}_l = \mathcal{F}\{P\} * \mathcal{F}\{G_l\}\] <p>where $\mathcal{F}(\cdot)$ represents the $d_l$ point DFT.</p> <p>From (1), the Fourier Transform of $P(x,y)$ is given by:</p> \[\mathcal{F}\{P(x, y)\}(u, v) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} P(x, y) e^{-i 2 \pi (u x + v y)} \, dx \, dy\] \[\quad \implies \mathcal{F}\{P(x, y)\}(u, v) = \int_0^{k_l} \int_0^{k_l} e^{-i 2 \pi (u x + v y)} \, dx \, dy \quad\] <p>$\text{Evaluating wrt x:}$</p> \[\int_0^{k_l} e^{-i 2 \pi u x} d x=\frac{1-e^{-i 2 \pi u k_l}}{i 2 \pi u}=k_l \operatorname{sinc}\left(\frac{u k_l}{d_l}\right)\] <p>$\text{Evaluating wrt y:}$</p> \[\int_0^{k_l} e^{-i 2 \pi v y} d y=\frac{1-e^{-i 2 \pi v k_l}}{i 2 \pi v}=k_l \operatorname{sinc}\left(\frac{v k_l}{d_l}\right)\] <p>$\text{Combining these results, the Fourier transform of P(x,y) is:}$</p> \[\mathcal{F}\{P(x, y)\}(u, v)=k_l^2 \operatorname{sinc}\left(\frac{u \kappa_l}{d_l}\right) \operatorname{sinc}\left(\frac{v k_l}{d_l}\right)\] <p>When the function is sampled, aliasing causes the spectrum to repeat periodically in the frequency domain. Each repetition of the sinc function at integer multiples of the sampling frequency creates a periodic aliasing pattern. In case of P(x,y) the function transforms into:</p> \[\operatorname{Sinc}(u, v)=\frac{\sin \left(\frac{\pi u k_l}{d_l}\right) \sin \left(\frac{\pi v k_l}{d_l}\right)}{\sin \left(\frac{\pi u}{d_l}\right) \sin \left(\frac{\pi v}{d_l}\right)} e^{-j \pi(u+v)\left(\frac{k_{l}-1}{d_l}\right)}\] <p>Here’s a breakdown of the components: <br/> $\sin(\frac{\pi u k_l}{d_l})$ : This is the sinc function scaled by the ratio of $k_l$ and $d_l$, which determines how the spatial box function in the spatial domain transforms in the frequency domain.</p> <p>The phase term $e^{-j \pi(u+v)\left(\frac{k_{l}-1}{d_l}\right)}$ : This accounts for a shift in the frequency domain. This phase shift arises due to the position of the box function in the spatial domain. This ensures that the Fourier transform reflects the correct location of the box function.</p> <p>Calculating for the correlation between $U$ and $V$:</p> \[\operatorname{Cov}[U, V]=\operatorname{Cov}\left[\operatorname{Sinc} * \mathcal{F}\left\{G_l\right\}\left(u_0, v_0\right), \operatorname{Sinc} * \mathcal{F}\left\{G_l\right\}\left(u_1, v_1\right)\right]\] <p>To expand this covariance term, we express $U$ and $V$ in terms of the sinc function and the frequency components of $G_l$:</p> \[\begin{aligned} U &amp; =\sum_{u, v} \operatorname{Sinc}(u, v) \cdot \mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right) \\ V &amp; =\sum_{\hat{u}, \hat{v}} \operatorname{Sinc}(\hat{u}, \hat{v}) \cdot \mathcal{F}\left\{G_l\right\}\left(u_1-\hat{u}, v_1-\hat{v}\right) \end{aligned}\] \[\implies \operatorname{Cov}[U, V]=\operatorname{Cov}\left(\sum_{u, v} \operatorname{Sinc}(u, v) \cdot \mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right), \sum_{\hat{u}, \hat{v}} \operatorname{Sinc}(\hat{u}, \hat{v}) \cdot \mathcal{F}\left\{G_l\right\}\left(u_1-\hat{u}, v_1-\hat{v}\right)\right)\] <p>This expands to: \(\operatorname{Cov}[U, V]=\sum_{u, v} \sum_{\hat{u}, \hat{v}} \operatorname{Sinc}(u, v) \operatorname{Sinc}^*(\hat{u}, \hat{v}) \cdot \operatorname{Cov}\left(\mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right), \mathcal{F}\left\{G_l\right\}\left(u_1-\hat{u}, v_1-\hat{v}\right)\right)\)</p> <p>Since $G_l$ is assumed to have independent frequency components (with variance $\sigma^2$ for each component), the covariance between any two distinct components is zero, while the variance of each component is $\sigma^2$. Therefore, the covariance term simplifies because we only need to consider the terms where $(u,v) = (\hat{u},\hat{v})$:</p> \[\operatorname{Cov}[U, V]=\sum_{u, v} \operatorname{Sinc}(u, v) \operatorname{Sinc}\left(u_0-u_1-u, v_0-v_1-v\right) \cdot \sigma^2\] <p>The final covariance expression simplifies further by factoring out $\sigma^2$ and recognizing the sum as a convolution:</p> \[\operatorname{Cov}[U, V]=\sigma^2 \sum_{u, v} \operatorname{Sinc}(u, v) \operatorname{Sinc}\left(u_0-u_1-u, v_0-v_1-v\right)\] <p>Using the definition of convolution, we get:</p> \[\operatorname{Cov}[U, V]=\sigma^2 \cdot \operatorname{Sinc} * \operatorname{Sinc}\left(u_0-u_1, v_0-v_1\right)\] <p>Since the sinc function is defined over the finite output space $d_l \times d_l$, the convolution integrates to $d_l^2$, giving us:</p> \[\operatorname{Cov}[U, V]=\sigma^2 d_l^2 \operatorname{Sinc}\left(u_0-u_1, v_0-v_1\right)\] <p>Next, we calculate the variance of $U$ (or similarly for $V$, due to symmetry) using the expression for $U$ from earlier. This is computed as:</p> \[\operatorname{Var}[U]=\operatorname{Var}\left(\sum_{u, v} \operatorname{Sinc}(u, v) \cdot \mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right)\right)\] <p>Using independence again, the variance simplifies to:</p> \[\operatorname{Var}[U]=\sum_{u, v}|\operatorname{Sinc}(u, v)|^2 \cdot \operatorname{Var}\left(\mathcal{F}\left\{G_l\right\}\left(u_0-u, v_0-v\right)\right)\] <p>Substituting the variance $\sigma^2$ of each independent component:</p> \[\operatorname{Var}[U]=\sigma^2 \sum_{u, v}|\operatorname{Sinc}(u, v)|^2\] <p>The sum over \(|Sinc(u,v)|^2\) evaluates to \(d_l^2k_l^2\), so:</p> \[\operatorname{Var}[U]=\sigma^2 d_l^2 k_l^2\] <p>Finally, we calculate the complex correlation coefficient between $U$ and $V$, which is defined as:</p> \[\operatorname{corr}(U, V)=\frac{\operatorname{Cov}[U, V]}{\sqrt{\operatorname{Var}[U] \operatorname{Var}[V]}}\] <p>Substituting,</p> \[\operatorname{corr}(U, V)=\frac{\sigma^2 d_l^2 \operatorname{Sinc}\left(u_0-u_1, v_0-v_1\right)}{\sqrt{\sigma^2 d_l^2 k_l^2 \cdot \sigma^2 d_l^2 k_l^2}}\] \[\implies \operatorname{corr}(U, V)=\frac{\operatorname{Sinc}\left(u_0-u_1, v_0-v_1\right)}{k_l^2}\] <p>Now, if the $U$ and $V$ frequencies are diagonally adjacent, then the correlation coefficient becomes:</p> \[\begin{equation} |\operatorname{corr}(U, V)|=\frac{\sin ^2\left(\frac{\pi k_l}{d_l}\right)}{k_l^2 \sin ^2\left(\frac{\pi}{d_l}\right)} \end{equation}\] <p>This result indicates that the correlation between two frequency components in the spectrum of $F_l$ is inversely related to the filter size $k_l$. A larger filter (i.e., higher $k_l$) reduces the correlation between frequencies, enhancing the filter’s ability to represent diverse frequencies independently. Conversely, a smaller filter (lower $k_l$) increases correlation, meaning that adjustments to one part of the frequency spectrum impact neighboring frequencies, thereby limiting the filter’s effective capacity to separate and individually adjust each frequency component.</p> <p>In each convolutional layer, the maximum spatial frequency that can be achieved is bounded by the Nyquist frequency. This means that a convolutional layer can accurately control spatial frequencies within the range $[0, \frac{d_l}{2d}]$ without aliasing. As a result, the high-frequency components are predominantly generated by the earlier layers of the CNN, which have larger spatial dimensions $d_l$. With a fixed filter size $k_l$, an increase in $d_l$ leads to higher correlations across the filter’s spectrum, thereby reducing the filter’s effective capacity to fine-tune individual frequencies. Consequently, earlier layers, responsible for creating high frequencies, face more restrictions in their spectral capacity compared to later layers with smaller $d_l$, which have greater flexibility for spectral adjustments.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Filter_Response-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Filter_Response-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Filter_Response-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Filter_Response.png" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Image Source: <a href="https://arxiv.org/pdf/2403.05093" target="_blank">'Spectrum Translation for Refinement of Image Generation (STIG) Based on Contrastive Learning and Spectral Filter Profile'</a> </div> <p>Moreover, while only the earlier layers can produce high frequencies without aliasing, all layers can contribute to the low-frequency spectrum without this restriction. Thus, the spatial extent of the effective filter acting on low frequencies is consistently larger than that acting on high frequencies. Even if larger filter sizes $k_l$ are used in the earlier layers to counterbalance the larger $d_l$ , low frequencies continue to benefit from a larger effective filter size compared to high frequencies, which ultimately results in lower correlation at low frequencies.</p> <p>In addition to this, some works<d-cite key="chen2020ssdganmeasuringrealnessspatial"></d-cite> show that downsampling layers also cause missing frequencies in discriminator. This issue may make the generator lacking the gradient information to model high-frequency content, resulting in a significant spectrum discrepancy between generated images and real images. Frameworks like STIG<d-cite key="lee2024spectrumtranslationrefinementimage"></d-cite> have been used to eliminate this bias.</p> <h2 id="frequency-bias-in-diffusion-models">Frequency bias in Diffusion Models</h2> <p>It has been well known that like GANs<d-cite key="goodfellow2014generativeadversarialnetworks"></d-cite>, diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> too show some frequency bias. <strong>Smaller models fail to fit the high frequency spectrum properly whereas larger models are succesful in doing so</strong><d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite>. In general, models have a hard time fitting the reduced spectrum graph especially where the magnitude of a particular frequency is low. This is shown in the graph below :</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectral_den_diff-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectral_den_diff-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectral_den_diff-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectral_den_diff.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Spectral density graphs of real and generated image of baboon. </div> <p>Diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> first fit the high magnitude parts (which correspond to the low frequency region in natural images). After fitting the low frequency region , it then fits the graph in the high frequency region(or low magnitude regions). Large models have enough parameters and timesteps to fit the high frequency region spectrum as well but small models struggle to do so due to lack of enough timesteps<d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite>. We shall see a modified and quite detailed version of the math proof from the paper ‘Diffusion Probabilistic Model Made Slim’<d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite> paper. We show that by taking the assumption that the denoising network acts as a linear filter, the math works out such that the reduced spectrum is first fitted for the low frequency(or high magnitude) region in the initial timesteps and later fitted for the high frequency (or low magnitude region). Assuming the denoising network as a linear filter, we get it to work as an optimal linear filter or Weiner filter. The function of this Weiner filter<d-footnote>refer to <a href="https://en.wikipedia.org/wiki/Wiener_filter" target="_blank">this article</a></d-footnote> is to minimize the mean squared error between the actual noise and the predicted noise by the filter.</p> <p>Let the input image that we want to reconstruct be $x_0$ and $\epsilon$ be white noise of variance 1. $x_t$ is the noised sample at time step t. Hence we can write</p> \[\mathbf{x}_t = \sqrt{\bar{\alpha}} \mathbf{x}_0 + \sqrt{1 - \bar{\alpha}} \epsilon\] <p>In the denoising process , let $h_t$ be the filter that is learned . So $h_t^* $ is the optimal filter which minimizes the loss function of the diffusion model<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> $L_t = |\mathbf{h}_t * \mathbf{x}_t - \epsilon|^2$ . Here $* $ denotes the standard convolution operation. The optimal filter solution can be derived in the frequency domain i.e. \(\mathcal{H}_t^*(f) = \frac{1}{\bar{\alpha} |X_0(f)|^2 + 1 - \bar{\alpha}}\). Here $\mathcal{H}_t^*(f)$ represents the frequency response of the filter. \(|X_0(f)|^2\) is the power spectrum of the original signal i.e. $X_0$, representing the magnitude of a particular frequency f in the spectrum. During the denoising phase $\bar{\alpha}$ goes from 0 to 1.</p> <p>Now if the optimal filter $\mathbf{h}_t$ is learned, then we approximate $\epsilon$ by $\mathbf{h}_t * \mathbf{x}_t$ so our noising step equation becomes:</p> \[x_t = \sqrt{\bar{\alpha}} \, x_0 + \sqrt{1 - \bar{\alpha}} \, (h_t * x_t)\] <p>Taking the DFT on both sides, we get</p> \[X_t = \sqrt{\bar{\alpha}} \, X_0 + \sqrt{1 - \bar{\alpha}} \, (H_t\times X_t)\] <p>Rearranging the equation, we get</p> \[\left( \frac{1 - \sqrt{1 - \bar{\alpha}}}{\sqrt{\bar{\alpha}}} H_t \right) X_t = X_0\] <p>Let $\left( \frac{1 - \sqrt{1 - \bar{\alpha}}}{\sqrt{\bar{\alpha}}} H_t \right)$ = $G_t$. Here $G_t$ is the frequency response of a filter $g_t$ which is the optimal linear reconstruction filter. Now this optimal filter minimises the equation :</p> \[J_t = \left| G_t X_t - X_0 \right|^2\] \[J_t = \left| G_t(\sqrt{\bar{\alpha}} \mathbf{X}_0 + \sqrt{1 - \bar{\alpha}} \epsilon )- X_0 \right|^2\] <p>The equation is approximately equal to</p> \[J_t \approx \left| X_0 \right|^2 \left| 1 - \sqrt{\overline{\alpha}} \, G_t \right|^2 + ({1 - \overline{\alpha}}) \, \left| \epsilon \right|^2 \left| G_t \right|^2\] <p>as $\epsilon$ and $X_0$ are uncorrelated. Here $\left| X_0 \right|^2$ is the power spectrum of $X_0$ and $\left| \epsilon \right|^2$ is the power spectrum of white noise which is equal to 1. So to find this optimal reconstruction filter, we differentiate this equation wrt $G_t$ and equate it to 0. We get,</p> \[\frac{\partial J_t}{\partial G_t} = 0 \implies \left| X_0 \right|^2 \left[ \left( 1 - G_t^* \sqrt{\overline{\alpha}} \right) \left( -\sqrt{\overline{\alpha}} \right) \right] + G_t^* (1 - \sqrt{\overline{\alpha}}) = 0\] <p>This gives us</p> \[G_t^* = \frac{\sqrt{\overline{\alpha}}}{\overline{\alpha} + \frac{1 - \overline{\alpha}}{|X_0|^2}}\] <p>Here $ G_t^* $ is the conjugate reconstruction filter. As it is real, $G_t^* = G_t$ . Hence, \(G_t = \frac{\sqrt{\overline{\alpha}}}{\overline{\alpha} + \frac{1 - \overline{\alpha}}{|X_0|^2}}\)</p> <p>is the optimal linear reconstruction filter. The predicted $\hat{X}_0$ = $G_t \times X_t$. So predicted power spectrum \(|\hat{X}_0|^2 = |G_t|^2 |X_t|^2\)</p> \[|X_t|^2 \approx \, \overline{\alpha} |X_0|^2 + (1 - \overline{\alpha}) |\epsilon|^2 = \overline{\alpha} |X_0|^2 + 1 - \overline{\alpha}\] <p>We can approximate it like This as $X_0$ and $\epsilon$ are uncorrelated. Now, let’s analyse the expression</p> \[|\hat{X_0}|^2 = |G_t|^2 |X_t|^2 = \frac{\overline{\alpha} \, \left| X_0 \right|^4}{\left( \overline{\alpha} \, \left| X_0 \right|^2 + 1 - \overline{\alpha} \right)^2} \, \left( \overline{\alpha} \, \left| X_0 \right|^2 + 1 - \overline{\alpha} \right)\] <p>Now, during the initial denoising stages, \(\bar{\alpha} \approx 0\). So in the low frequency region, $|X_0|^2$ is very high($|X_0|^2 \gg 1$). We make the assumption that \({\overline{\alpha}} \, |X_0| \approx 1\). So in the low frequency region, \(|\hat{X_0}|^2 \approx |X_0|^2\). In the high frequency region, $|X_0|^2$ is very low ($|X_0|^2 \to 0$). So, \(|\hat{X_0}|^2 \approx 0\). It can be clearly seen that in the inital denoising steps, the high magnitude signal is reconstructed while the low magnitude signal is approximated to zero.</p> <p>In the later stages of the denoising process, \(\bar{\alpha} \approx 1\), so regardless of the magnitude of \(|X_0|^2\), the value of \(|\hat{X_0}|^2 \approx |{X_0}|^2\)</p> <p>So we can clearly see that the model is succesfully able to learn the low frequency content in its initial denoising steps and eventually, given enough time steps, it learns the entire spectrum. But small models lack enough time steps and parameters, so only the low frequency spectrum is learnt well by the model and the predicted high frequency content is less than the ground truth. Note that the proof is based on the fact that the model has a hard time learning low magnitude regions in the power spectrum, which correspond to high frequency in natural images. But if we take a synthetic image which has low magnitude in the middle frequency region and high magnitude in the low and high frequency region, then as expected, the model fails to fit the middle region of the reduced spectrum properly. This can be seen below when we try to fit a synthetic image with two gaussian peaks by a small diffusion model<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> with limited timesteps.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectrum_syn-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectrum_syn-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectrum_syn-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/spectrum_syn.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>The model first fits the high magnitude regions and then tries to fit the low magnitude regions , which it fails to do as the number of timesteps are small.</p> <p>There is another reason which might contribute to this bias. It is because the loss of a DDPM takes an expectation over the dataset.</p> \[\mathcal{L}_{\text{DDPM}} = \int p(\mathbf{x}_0) \, \mathbb{E}_{t, \epsilon} \left[ \left\| \epsilon - s(\mathbf{x}_t, t; \theta) \right\|_2^2 \right] d\mathbf{x}_0\] <p>Most images have smooth features and there is a small perecntage of samples have high frequency components, hence p($\mathbf{x}_0$) for such samples is low and they are down weighted in the loss function<d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite>. Due to their low weight, not much importance is given to reconstruction of high frequency components.</p> <h2 id="mitigation-of-frequency-bias-using-spectral-diffusion-model">Mitigation of Frequency Bias Using Spectral Diffusion Model</h2> <p>The main problem with diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> is that the small vanilla U-Net cannot incorporate the dynamic spectrum into its loss function. So, the authors of the paper<d-cite key="yang2022diffusionprobabilisticmodelslim"></d-cite> introduce a spectrum-aware distillation to enable photo-realistic generation with small models. The U-Net is replaced with a Wavelet Gating module which consists of a <strong>WG-Down</strong> and <strong>WG-Up network</strong>. The WG-Down network takes the Discrete Wavelet Transform(DWT)<d-footnote>refer to this <a href="https://en.wikipedia.org/wiki/Discrete_wavelet_transform" target="_blank">article</a></d-footnote> of the input image and outputs 4 images of sub-bands. They are respectively the LL, LH, HL, HH sub-bands. In the LL sub-band,a low-pass filter is applied on the rows and columns of the image and thus captures most of the low-frequency content of the image. The LH band is created by passing a low-pass filter on the rows and high-pass filter on the columns. The HL band is created by passing a high-pass filter on the rows and a low-pass filter on the columns of the image.Finally, the HH sub-band is created by passing a high-pass filter on both rows and columns. In essence, the LL sub-band captures the low-frequency details i.e. an approximation of the image, while the LH, HL, HH sub-bands capture the high-frequency details of the image.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Spectral_diffusion-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Spectral_diffusion-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Spectral_diffusion-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analysing-the-spectral-biases-in-generative-models/Spectral_diffusion.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"> Image Source: <a href="https://arxiv.org/abs/2211.17106" target="_blank">'Diffusion Probabilistic Model Made Slim'</a> </div> <p>The input image $X$ of size $H \times W \times C$ is divided into its corresponding 4 sub-bands (each of size $H/2 \times W/2 \times C$). Next, a soft-gating operation is used to weight these 4 sub-bands and the output feature $X’$ is produced as follows:</p> \[X' = \sum_{i \in \{LL, LH, HL, HH\}} g_i \odot X_i\] <p>Here, $\odot$ represents element-wise multiplication. The gating mask is learnt using a feed-forward network.</p> \[g_{\{LL, LH, HL, HH\}} = \text{Sigmoid}(\text{FFN}(\text{Avgpool}(\mathbf{X})))\] <p>In the WG-Up, the input feature is splitted into 4 chunks as the wavelet coefficients. Then, WG is carried out to re-weight each sub-band as before:</p> \[X' = \text{IDWT}(g_{LL} \odot X_{LL}, g_{LH} \odot X_{LH}, g_{HL} \odot X_{HL}, g_{HH} \odot X_{HH})\] <p>Here IDWT is the inverse Discrete wavelet transform. Hence, they provide the model information of the frequency dynamics as well during training as the network decides which components to pay more attention to while learning the gating mask.</p> <p>Another method that the authors apply is spectrum-aware distillation. They distill knowledge from a large pre-trained model into the WG-Unet. They distill both spatial and frequency knowledge into the WG-Unet using a spatial loss and a frequency loss. Let a noised image $X_t$ be passed into the network. The spatial loss is calculated as:</p> \[\mathcal{L}_{\text{spatial}} = \sum_{i} \| \mathbf{X}^{(i)}_T - \mathbf{X}^{(i)}_S \|^2_2\] <p>where $\mathbf{X}^{(i)}_T$ and $\mathbf{X}^{(i)}_S$ stand for the pair of teacher/student’s output features or outputs of the same scale. A single $1 \times 1$ <strong>Conv</strong> layer is used to align the dimensions between a prediction pair.</p> <p>The frequency loss is then calculated as :</p> \[\mathcal{L}_{\text{freq}} = \sum_i \omega_i \left\| \mathcal{X}_T^{(i)} - \mathcal{X}_S^{(i)} \right\|_2^2, \quad \text{where } \omega = \left| \mathcal{X}^{(i)} \right|^{\alpha}\] <p>Here, $\mathcal{X}_T^{(i)}$, $\mathcal{X}_S^{(i)}$ and $\mathcal{X}_0^{(i)}$ represent the 2D DFT of $\mathbf{X}^{(i)}_T$, $\mathbf{X}^{(i)}_S$ and the resized clean image $X_0$ respectively. The scaling factor $\alpha$ is $-1$. We multiply the loss with $\omega_i$ as it gives more weight to high frequency content ($\mathcal{X}_0^{(i)}$ is low, hence $\omega_i$ is high) and less weight to low-frequency content.</p> <p>So the final loss is:</p> \[\mathcal{L} = \mathcal{L}_{\text{DDPM}} + \lambda_s \mathcal{L}_{\text{spatial}} + \lambda_f \mathcal{L}_{\text{freq}}\] <p>Here $\lambda_s$ = 0.1 and $\lambda_f$ = 0.1.</p> <p>It was found that the frequency term in the loss function accounts for the largest change in FID, showing its importance in high-quality image generation. Thus using spectral knowledge during training helps a small network to produce high-quality realistic images and eliminate the ‘bias’ problem in diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite>.</p> <h1 id="conclusion">Conclusion</h1> <p>In this article, we looked at another domain of viewing images and processing them i.e. the frequency domain. We then shed light into how the generative models posses certain biases in the frequency domain, particularly bias against high frequency content generation. We try to explain the reason behind this by breaking down the architecure of GANs<d-cite key="goodfellow2014generativeadversarialnetworks"></d-cite> and diffusion models<d-cite key="ho2020denoisingdiffusionprobabilisticmodels"></d-cite> and look at how the math behind these model’s working may lead to these observations. Finally, we discussed an architecture to mitigate these issues.</p>]]></content><author><name>Amitoj Singh Miglani</name></author><summary type="html"><![CDATA[Diffusion and GAN models have demonstrated remarkable success in synthesizing high-quality images propelling them into various real-life applications across different domains. However, it has been observed that they exhibit spectral biases that impact their ability to generate certain frequencies and makes it possible to distinguish real images from fake ones. In this blog we analyze these models and attempt to explain the reason behind these biases.]]></summary></entry><entry><title type="html">A primer on analytical learning dynamics of nonlinear neural networks</title><link href="https://blogpostabc.github.io/ai810/blog/analytical-simulated-dynamics/" rel="alternate" type="text/html" title="A primer on analytical learning dynamics of nonlinear neural networks"/><published>2025-04-28T00:00:00+09:00</published><updated>2025-04-28T00:00:00+09:00</updated><id>https://blogpostabc.github.io/ai810/blog/analytical-simulated-dynamics</id><content type="html" xml:base="https://blogpostabc.github.io/ai810/blog/analytical-simulated-dynamics/"><![CDATA[<h2 id="background">Background</h2> <p>The dynamics of learning in artificial neural networks capture how the parameters of a network change over time during training as a function of the data, architecture, and training algorithm. The internal representations and external behaviors of the network evolve during training as a consequence of these dynamics. In neural networks with at least one hidden layer, these dynamics are <em>nonlinear</em> even when the activation function is linear <d-cite key="saxe2014exact"></d-cite>, making them challenging to characterize even qualitatively. Nevertheless, understanding learning dynamics is central to machine learning research, as they determine how networks acquire useful features that generalize to unseen data through the data-driven optimization process of gradient descent on the training objective.</p> <h3 id="empirical-studies-of-learning-dynamics">Empirical studies of learning dynamics</h3> <p>Empirical work uses simulations of training dynamics to visualize and characterize the geometry of neural network loss landscapes and the trajectories networks take during optimization. Early work by Choromanska et al. <d-cite key="choromanska2015loss"></d-cite> and Goodfellow et al. <d-cite key="goodfellow2015qualitatively"></d-cite> challenged the notion that local minima were a significant obstacle to optimization, finding that most local minima have similar loss values. Garipov et al. <d-cite key="garipov2018loss"></d-cite> further showed that different solutions are often connected by simple paths in parameter space. Visualization techniques developed by Li et al. <d-cite key="li2018visualizing"></d-cite> and Sagun et al. <d-cite key="sagun2018empirical"></d-cite> revealed the structure of these landscapes, showing how network width and depth affect the loss geometry. Fort et al. examined how networks traverse these landscapes <d-cite key="fort2019goldilocks"></d-cite> and characterizing the role of network width in determining training trajectories <d-cite key="fort2020deep"></d-cite>. More recent work by Entezari et al. <d-cite key="entezari2022rolea"></d-cite> has worked connected these empirical observations to theoretical frameworks for understanding optimization dynamics.</p> <h3 id="theoretical-analyses-of-learning-dynamics">Theoretical analyses of learning dynamics</h3> <p>Theoretical approaches to understanding neural network learning dynamics use mathematical tools to describe how network parameters evolve during training. These analyses have revealed two distinct training regimes. The first regime, known as <em>lazy training</em> <d-cite key="chizat2019lazy"></d-cite>, occurs when network parameters stay close to their initialization throughout training. In this regime, the network behaves similarly to a kernel method, with dynamics characterized by the <em>neural tangent kernel</em> <d-cite key="jacot2020neural"></d-cite><d-cite key="arora2019finegrained"></d-cite><d-cite key="allen-zhu2020learning"></d-cite>.</p> <p>The second regime, termed <em>feature learning</em> <d-cite key="yang2022feature"></d-cite>, captures more complex dynamics in which networks substantially modify their internal representations during training as a function of the task. Even seemingly simple architectures like deep linear networks can exhibit rich feature learning dynamics, including distinct learning phases where different hierarchical features emerge rapidly followed by plateaus of little progress <d-cite key="saxe2014exact"></d-cite>. The transition between the rich and lazy regimes depends on the interplay between factors such as the network width, learning rate, and initialization scale <d-cite key="yang2023spectral"></d-cite>, and the dynamics can transition between these regimes during training <d-cite key="kunin2024get"></d-cite>, resulting in drastic changes in generalization behavior <d-cite key="kumar2023grokking"></d-cite>. The feature learning regime is particularly relevant for understanding the success of deep learning in practice, where networks learn representations of complex data distributions with an effectiveness that is not yet fully understood.</p> <h3 id="statistical-physics-for-learning-dynamics">Statistical physics for learning dynamics</h3> <p>Statistical physics offers tools for characterizing macroscopic behavior emerging from collections of microscopic particles <d-cite key="helias2019statistical"></d-cite> <d-cite key="urbani2024statistical"></d-cite>. Early pioneering work by Gardner applied these techniques to neural networks by viewing neurons as the microscopic particles in a complex system—the neural network <d-cite key="gardner1989three"></d-cite>. The primary goal of many statistical physics approaches to learning dynamics is to derive an exact equation for time-varying generalization error through reduction to macroscopic variables to be defined. These analyses can be performed under various neural network parameter regimes, often considering asymptotic limits (such as infinite width or infinite input dimension) where the system concentrates—exhibiting fewer fluctuations in a precise sense—leading to simpler dynamical descriptions; see Cui <d-cite key="cui2024highdimensional"></d-cite> for a review of these regimes.</p> <h4 id="our-focus-the-teacher-student-setting">Our focus: The teacher-student setting</h4> <p>The teacher-student framework, introduced by Gardner <d-cite key="gardner1989three"></d-cite>, provides perhaps the simplest setting for studying neural network learning with statistical physics techniques. In this paradigm, a student network learns to mimic a fixed teacher network that generates labels for training data drawn from a given input distribution. Classical analytical results were achieved by Saad &amp; Solla <d-cite key="saad1995online"></d-cite> and Riegler and Biehl <d-cite key="riegler1995online"></d-cite> in the 1990s, who derived exact equations for the generalization dynamics in this teacher-student setup with a particular scaling. To enable solvability, these analyses require specific assumptions about the data distribution and the network architecture—in particular, that the network inputs are Gaussian and that the input dimension is large relative to the number of hidden neurons. Despite these constraints, this framework allow granular study of various learning regimes, including overparameterization (termed <em>realizability</em> in Saad &amp; Solla <d-cite key="saad1995online"></d-cite>) and a feature-learning phenomena termed <em>specialization</em>.</p> <p>Much recent work builds on these classical analyses to expand the frontier of solvable training regimes, and these techniques have also found applications beyond generalization error dynamics. We detail both directions in the final section of this blog post. First, we provide a pedagogical introduction to the classicial analytical results, focusing on the derivation of Saad &amp; Solla <d-cite key="saad1995online"></d-cite> for its clarity.</p> <h2 id="methods">Methods</h2> <p>In this section, we introduce the teacher-student setup of Saad &amp; Solla <d-cite key="saad1995online"></d-cite> to prepare for the next section, where we rederive the analytical learning dynamics of the student network. To complement the derivations, we include code snippets for computing the time-evolution of the macroscopic variables describing the learning dynamics efficiently in JAX <d-cite key="jax2018github"></d-cite>, which we use in the next-after-next section to test the theory-experiment overlap of the generalization error dynamics targeted by Saad &amp; Solla <d-cite key="saad1995online"></d-cite>.</p> <h3 id="the-teacher-student-setup">The teacher-student setup</h3> <p>In the teacher-student setting of Saad &amp; Solla <d-cite key="saad1995online"></d-cite>, the data-generating process used to train the student network is described by a distrubution over inputs $x$ and a teacher providing target outputs $y$. Saad &amp; Solla <d-cite key="saad1995online"></d-cite> focus on the online learning (also termed <em>stochastic</em> gradient descent) setting, where new samples are drawn from the data-generating process at random. In this setting, a batch size greater than one has no substantial effect on the dynamics except to reduce the noise in the gradient estimate. As such, in simulations we use minibatch stochastic gradient descent and sample multiple $(x_{s}, y_{s})^{u}$ pairs to fill a batch $s = 1, \ldots, B$. and we consider updates of the student network using gradient descent iterations indexed by $u$. As such, in the case of \(B=1\), \(u\) can be understood simultanesouly as the iteration index in the training process and the example index (<em>i.e.</em>, the number of examples seen so far in the training process).</p> <p>The teacher, including that of Saad &amp; Solla <d-cite key="saad1995online"></d-cite>, is generally defined as</p> \[\begin{equation} y_{s}^{u} = f^{*}(x_{s}^{u}, W^{*}) + \sigma \xi^{u}~, \end{equation}\] <p>where \(f^{*}( \cdot , W^{*})\) is the mapping defining the teacher with parameters \(W^{*}\) of dimension to be defined, \(\xi^{u} \sim \mathcal{N}(0, 1)\), and \(\sigma \in \mathbb{R}\) scales the output noise.</p> <p>The student network is generally defined as</p> \[\begin{equation} \hat{y}_{s}^{u} = f(x_{s}^{u}, W), \end{equation}\] <p>where \(f( \cdot , W)\) is the mapping defining the student, and \(W\) are the parameters of the student of dimension to be defined. In teacher-student, \(f\) and \(f^{*}\) usually share the same parameterizaton to enable finegrained comparison between the learning of the student and the teacher defining the task. Commonly, they are both neural networks and the parameters \(W\) and \(W^{*}\) are the weights of the networks. In Saad &amp; Solla <d-cite key="saad1995online"></d-cite>, both the teacher and student networks were modeled as a soft-committee machine, which is a sum of nonlinear perceptrons. As such, the student and teacher networks had parameters of dimension \(W \in \mathbb{R}^{K \times N}\) and \(W^{*} \in \mathbb{R}^{M \times N}\), respectively, where \(K\) and \(M\) are the number of neurons in the student and teacher networks, respectively.</p> <p>To train the student network, Saad &amp; Solla <d-cite key="saad1995online"></d-cite> consider gradient descent to improve on the mean squared error between teacher and student outputs at iteration $u$:</p> \[\begin{equation} \mathcal{L}^{u} = \frac{1}{2B} \sum_{s=1}^{B} \left( \hat{y}_{s}^{u} - y_{s}^{u} \right)^{2}~, \end{equation}\] <p>where samples to fill a batch consist of $(x_{s}, y_{s})^{u}$ pairs with $i = 1, \ldots, B$, $x_{s}^{u} \sim \mathcal{N}(0, I)$ of dimension $N$, and the target $y_{s}^{u}$ is generated by feeding $x_{s}^{u}$ to the teacher network. The weights of the student network are updated using gradient descent as</p> \[\begin{equation} W^{u+1} = W^{u} - \eta_{w} \frac{\partial \mathcal{L}^{u}}{\partial W}~, \end{equation}\] <p>where $\eta_{W}$ is the learning rate for parameters \(W\). This procedure can be shown to converge to near-zero training error (up to irreducible error due to noise ) in the limit of infinite data and infinitesmal learning rate if the student is sufficiently parameterized with respect to the teacher <d-cite key="saad1995online"></d-cite>.</p> <h3 id="gradient-flow-dynamics-learning-with-infinitesimal-step-size">Gradient flow dynamics: Learning with infinitesimal step size</h3> <p>One way to analyze learning dynamics of neural networks like that denoted in (4) is treat the optimization process as a dynamical system where the gradient descent updates effectively evolve through continuous time as the parameters of a dynamical system. This transformation is commonly known as the <em>gradient flow limit</em> <d-cite key="bach2020effortless"></d-cite>, where the discrete gradient descent updates become continuous when the learning rate is small, giving</p> \[\begin{equation} \frac{\mathrm{d}W}{\mathrm{d}t} = - \left\langle \frac{\partial \mathcal{L}^{u}}{\partial W} \right\rangle_{x,y} \end{equation}\] <p>where \(\left\langle \cdot \right\rangle_{x,y}\) is physics notation for the expectation taken over the distribution of the data. In the gradient flow limit, the generalization error at each step can be written as</p> \[\begin{equation} \mathcal{E}(W^u) = \frac{1}{2} \left\langle \left( \hat{y}^u - y \right)^{2} \right\rangle_{x,y}~. \end{equation}\] <p>One way to think about the limit defined by (5) is by considering that as the learning rate gets smaller, the amount of data observed by the network at a fixed timescale increases, becoming virtually infinite when the learning rate is zero. This converts the finite average over data in the loss function in (3) to an expectation over the data as in (6).</p> <h4 id="the-nonlinear-gradient-flow-dynamics-of-teacher-student-are-solvable">The nonlinear gradient flow dynamics of teacher-student are solvable</h4> <p>It is possible to solve the system of ordinary differential equations in (5) for several classes of deep linear networks <d-cite key="saxe2014exact"></d-cite> <d-cite key="braun2022exact"></d-cite> <d-cite key="shi2022learning"></d-cite> <d-cite key="tu2024mixed"></d-cite> at finite width. Happily, using a teacher-student setup allows for the derivation of a closed-form expression of the learning dynamics, even for <em>nonlinear</em> networks with a hidden layer. To achieve this, the above differential equation can be written in terms of specific <strong>order parameters</strong>, which sufficiently describe the state of the learning dynamics at each time step. Order parameters are commonly understood in physics as macroscopic variables that describe the time evolution of a complex system with many microscopic parts, in a way that is convenient for further mathematical analysis.</p> <p>In the next sections, we will rederive the dynamical equations for two paradigmatic cases of the teacher-student setting, the classical case of Saad and Solla <d-cite key="saad1995online"></d-cite> where the teacher and student are soft-committee machines (an average of nonlinear perceptrons), and that of Goldt et al. <d-cite key="goldt2020dynamics"></d-cite>, which extends these results to allow for nonlinear neural networks with a hidden layer.</p> <h2 id="rederivations">Rederivations</h2> <p>In this section, we present a pedagogical tour of the analytical framework of to characterize the gradient descent learning dynamics of the student network in terms of its order parameters, resolving some inconsistencies in the original derivations <d-cite key="saad1995online"></d-cite> and in follow-up extensions <d-cite key="goldt2020dynamics"></d-cite>.</p> <h3 id="solvable-learning-dynamics-for-the-soft-committee-machine">Solvable learning dynamics for the soft committee machine</h3> <p>To solve the system of ordinary differential equations in (5), we need to assume a specific form for the teacher and student networks and convert the dynamical equation that describes gradient descent to the corresponding order parameters equations. In Saad and Solla’s work <d-cite key="saad1995online"></d-cite>, both the teacher and student networks were modeled as a soft-committee machine, which is an average of nonlinear perceptrons. We define the teacher and student networks as follows, using modified notation for clarity:</p> \[\begin{equation} y_{s}^{u} = \sum_{m=1}^{M} g\left( \frac{W^{*}_{m} x_{s}^{u}}{\sqrt{N}} \right) + \sigma \xi^{u} \end{equation}\] \[\begin{equation} \hat{y}_{s}^{u} = \sum_{k=1}^{K} g\left( \frac{W_{k} x_{s}^{u}}{\sqrt{N}} \right) \end{equation}\] <p>where $g( \cdot )$ is the activation function, \(m\) and \(k\) index the perceptrons in the teacher and student (rows of \(W^*, W \in \mathbb{R}^{1 \times N}\)), and $M$ and $K$ are the number of neurons in the teacher and student networks, respectively. Saad and Solla <d-cite key="saad1995online"></d-cite> present closed-form solutions for $g( \cdot )$ as a <a href="https://en.wikipedia.org/wiki/Error_function">Gauss error function</a> nonlinearity. From here on, neuron indexing will be $i, j, k$ for the student, and $m, n, p$ for the teacher.</p> <p>To train the student, we minimize the mean squared error between the teacher and student outputs:</p> \[\begin{align} \mathcal{L^{u}} = &amp; \frac{1}{2B} \sum_{i=1}^{B} \left( y_{s}^{u} - \hat{y}_{s}^{u} \right)^{2} \\ = &amp; \frac{1}{2B} \sum_{s=1}^{B} \left[ \sum_{m=1}^ {M} g\left( \frac{W^{*}_{m}x_{s}^{u}}{\sqrt{N}} \right) + \sigma \xi^{u} -\sum_{k=1}^{K} g\left( \frac{W_ {k}x_{s}^{u}}{\sqrt{N}} \right) \right]^{2} \end{align}\] <p>We then perform gradient descent to update the student’s weights:</p> \[\begin{align} \frac{\partial \mathcal{L}}{\partial W_{i}} = &amp; \frac{1}{B}\sum_{s=1}^{B} \left[\sum_{m=1}^ {M} g\left( \frac{W^{*}_{m}x_{s}^{u}}{\sqrt{N}} \right) + \sigma \xi ^{u} -\sum_{k=1}^{K} g\left( \frac{W _{k}x_{s}^{u}}{\sqrt{N}} \right) \right] \cdot \left( -g'\left( \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}} \right) \\ = &amp; - \frac{1}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}}~. \end{align}\] <p>with \(\Delta_{s}^{u} = \sum_{m=1}^{M} g\left( \frac{W^{*}_{m}x_{s}^{u}}{\sqrt{N}} \right) + \sigma \xi^{u} - \sum_{k=1}^{K} g\left( \frac{W_{k}x_{s}^{u}}{\sqrt{N}} \right)\). Hence, the gradient descent update equations for the student network are</p> \[\begin{equation} W_{i}^{u+1} = W_{i}^{u} + \frac{\eta_{w}}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \frac{W_{i}x_ {s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}}~. \end{equation}\] <p>From this expression, we could take the gradient flow limit as in (5). However, the expectation over the data distribution induced in the right hand side does not have a closed form solution in this case. Instead, we can write the update equation in terms of <strong>order parameters</strong>, which fully define the state of the system, and for which this expectation has a solution. We emphasize that the order parameters are a design choice to make analytical results tractable, and the choice of order parameters is not always obvious in any given problem. Saad &amp; Solla <d-cite key="saad1995online"></d-cite> choose the order parameters to be the overlap between student and teacher neurons $R$, the overlap of student neurons with themselves $Q$, and the overlap of teacher neurons with themselves $T$ (which do not change throughout training as the teacher is fixed), defined as</p> \[\begin{equation} R = \frac{W^{*}W^{T}}{N}, \hspace {0.2cm} Q = \frac{W W^{T}}{N} \hspace{0.2cm} \text{and} \hspace{0.2cm} T = \frac{W^{*}(W^{*})^{T}}{N}. \end{equation}\] <p>Instead of describing the learning using the gradient descent updates for the weights, we can describe it in terms of the order parameters in (14). To do this, we can simply multiply the gradient updates equation by \((W^{*}_{n})^{T}/N\) to obtain $R$ updates and by \((W_{j}^{u+1})^{T}/N\) to obtain $Q$ updates. Starting with the $R$ updates, we have</p> \[\begin{align} \frac{W_{i}^{u+1}(W_{n}^{*})^{T}}{N} &amp; = \frac{W_{i}^{u}(W_{n}^{*})^{T}}{N} + \frac{\eta_{w}}{NB}\sum_{s= 1}^{B} \Delta_{s}^ {u} \cdot g'\left( \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u} (W_{n}^{*})^{T}}{\sqrt{N }}~, \\ R_{in}^{u+1} &amp; = R_{in}^{u} + \frac{\eta_{w} \mathrm{d}t}{B}\sum_{s=1}^{B} \Delta_{s}^ {u} \cdot g'\left( \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u} (W_{n}^{*})^{T}}{\sqrt{N}}~. \end{align}\] <p>From this equation, we define $\mathrm{d}t=1/N$, and by moving $R_{in}^{u}$ to the left hand side, dividing by $\mathrm{d}t$, and taking the <em>thermodynamic limit</em> $N \rightarrow \infty$ corresponding to large input dimension, we obtain the time derivative of $R_{in}$ as</p> \[\begin{equation} \frac{\mathrm{d} R_{in}}{\mathrm{d} t} = \eta_{w} \left&lt; \Delta_{s}^{u} g'(\lambda_{i}^{u}) \rho_{n}^{u} \right&gt; \end{equation}\] <p>where we define the <em>local fields</em></p> \[\begin{equation} \lambda_{i}^{u} = \frac{W_{i}x_{s}^{u}}{\sqrt{N}} \hspace{0.3cm} \text{and} \hspace{0.3cm} \rho_{n}^{u} = \frac{(W_ {n}^{*})^{T}x_{s}^{u}}{\sqrt{N}}. \end{equation}\] <p>The equation for $\frac{\mathrm{d}R_{in}}{\mathrm{d}t}$ is now in a convenient form, where the local fields are simply a Gaussian scalar since $x \sim \mathcal{N}(0, I)$, and so the expectation because an integral over Gaussian distribution with covariances defined by the order parameters. Before solving this expectation, let’s derive the same equation for the order parameters $Q$ (slightly trickier). We go back to the gradient descent update equation for the weights, and multiply by $(W_{j}^{u+1})^{T}/N$ giving</p> \[\begin{align} \frac{W_{i}^{u+1}(W_{j}^{u+1})^{T}}{N} &amp; = \frac{W_{i}^{u}(W_{j}^{u+1})^{T}}{N} + \frac{\eta_{w}}{NB}\sum _{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \lambda_{i}^ {u} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}}(W_{j}^{u+1})^{T}, \\ Q^{u+1}_{ij} &amp; = \frac{W_{i}^{u}}{N}\left( W_{j}^{u} + \frac{\eta_{w}}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left ( \frac{W_{j}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}} \right)^{T} \\ &amp; + \frac{\eta_{w}}{NB}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \lambda_{i}^ {u} \right) \cdot \frac{ x_{s}^{u}} {\sqrt{N}} \left( W_{j}^{u} + \frac{\eta_{w}}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left ( \frac{W_{j}x_{s}^{u}}{\sqrt{N}} \right) \cdot \frac{x_{s}^{u}}{\sqrt{N}} \right)^{T}, \\ Q^{u+1}_{ij} &amp; = Q^{u}_{ij} + \frac{\eta_{w}dt}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \lambda_{j }^ {u} \right) \lambda_{i}^{u} + \frac{\eta_{w}dt}{B}\sum_{s=1}^{B} \Delta_{s}^{u} \cdot g'\left( \lambda_{i}^{u } \right) \lambda_{j}^{u} \\ &amp; + \frac{\eta_{w}^{2}dt}{B^{2}}\sum_{s=1}^{B}\sum_{s'=1}^{B} \Delta_{s}^{u} \Delta_{s'}^{u}g'\left( \lambda_{i}^{u} \right)g'\left( \lambda_{j}^{u} \right) \frac{x_{s}^{u}(x_{s}^{u})^{T}}{N}. \end{align}\] <p>Now dividing by $\math{d}t$ and taking the limit $N \rightarrow \infty$, (hence $\mathrm{d}t \rightarrow 0$), $\frac{x_{s }^{u}(x_{s} ^{y})^{T}}{N} \rightarrow 1 $ by the central limit theorem, and expectations over \(s\) and \(s'\) are $0$ as they are independent samples, we obtain the time derivative of $Q_{ij}$ as</p> \[\begin{equation} \frac{\mathrm{d}Q_{ij}}{\mathrm{d}t} = \eta_{w} \left&lt; \Delta_{s}^{u} g'(\lambda_{j}^{u}) \lambda_{i}^{u} \right&gt; + \eta_{W } \left&lt;\Delta_{s}^{u} g'(\lambda_{i}^{u}) \lambda_{j}^{u} \right&gt; + \eta_{w}^{2} \left&lt; (\Delta_{s}^{u})^{2} g'(\lambda_{i}^{u}) g'(\lambda_{j}^{u}) \right&gt;. \end{equation}\] <p>Finally, having the order parameters, we can write the generalization error from (6), the expected mean squared error between teacher and student over the entire data distribution, as</p> \[\begin{align} \mathcal{E} = &amp; \frac{1}{2} \left&lt; \left( \hat{y} - y \right)^{2} \right&gt; \\ = &amp; \frac{1}{2} \sum_ {i=1}^{K} \sum_{j=1}^{K} \left&lt; g(\lambda_{i}^{u}) g(\lambda_{j}^{u}) \right&gt; - \sum_{m=1}^{M} \sum_{i=1}^{K} \left&lt; g(\rho_{m}^{u}) g(\lambda_{i}^{u}) \right&gt; + \frac{1}{2} \sum_{m=1}^{M} \sum_{n=1}^{M} \left&lt; g(\rho_{m}^{u}) g(\rho_ {n}^{u}) \right&gt; + \frac{\sigma^{2}}{2} \end{align}\] <p>Here, we encounter the first expectation that can be integrated in closed form for $g$ defined as the error function. Now, we introduce the useful expectations that will appear in the computation of expectations in the generalization error and order parameters:</p> \[\begin{align} I_{2}(a, b) &amp; = \left&lt; g(\nu_{a}) g(\phi_{b}) \right&gt; \\ I_{3}(a, b, c) &amp; = \left&lt; g'(\nu_{a}) \phi_{b} g(\psi_{c}) \right&gt; \\ I_{4}(a, b, c, d) &amp; = \left&lt; g'(\nu_{a}) g'(\phi_{b}) g(\psi_{c}) g(\gamma_{d}) \right&gt; \\ J_{2}(a, b) &amp; = \left&lt; g'(\nu_{a}) g'(\phi_{b}) \right&gt;~. \end{align}\] <p>These expectations can be solved in closed form as a function of the covariance between each of the variables $\nu_ {a}, \phi_{b}, \psi_{c}$ and $\gamma_{d}$. Let’s start with an example from the terms in the generalization error. First, closed form expression for $I_{2}$, with $g$ as the Gauss error function, is</p> \[\begin{align} I_{2}(a, b) = &amp; \frac{2}{\pi} \text{arcsin}\left( \frac{C_{ab}}{\sqrt{1 + C_{aa}} \sqrt{1+C_{bb}}} \right) \end{align}\] <p>where $C_{ab}$ is the covariance between $\nu_{a}$ and $\phi_{b}$, and $C_{aa}$ and $C_{bb}$ are the variances of $\nu_{a}$ and $\phi_{b}$, respectively. The way to select the correct covariance structure, is to look at the arguments of the expectation. Recall the index notation, $i, j, k$ for the student, and $m, n, p$ for the teacher, then $a$ and $b$ can be any of these indices depending on the corresponding local field. For instance, if $a=k$, the notation implies that $\nu = \lambda$, if $b=m$, then $\phi = \rho$. From here, we can write the generalization error in terms of this integral</p> \[\begin{align} \mathcal{E} = &amp; \frac{1}{2} \sum_{i=1}^{K} \sum_{j=1}^{K} I_{2}(i, j) - \sum_{n=1}^{M} \sum_{i=1}^{K} I_{2}(i, n)+ \frac{1}{2} \sum_{m=1}^{M} \sum_{n=1}^{M} I_{2}(n, m) + \frac{\sigma^{2}}{2}~. \end{align}\] <p>The covariances between the local fields are defined by the order parameters. For instance, the covariance for \(I_{2} (i, n)\) (student-teacher indexes) is \(C_{12} = R_{in}\), \(C_{11}=\text{diag}(Q)_{i}\) and \(C_{22}=\text{diag}(T)_{n}\), or the covariance for \(I_{2}(i, j)\) is \(C_{12}=Q_{ij}\), \(C_{11}=\text{diag}(Q)_{i}\) and \(C_{22}=\text{diag}(Q)_{j}\). In other words, the covariance structure for these expectation is given by the local field covariance, which are the order parameters. Hence, we can take advantage of broadcasting to compute all elements of \(I_{2}\) matrix as a function of the corresponding order parameters matrices:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_I2</span><span class="p">(</span><span class="n">c12</span><span class="p">,</span> <span class="n">c11</span><span class="p">,</span> <span class="n">c22</span><span class="p">):</span>
    <span class="n">e_c11</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">c11</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">e_c22</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">jnp</span><span class="p">.</span><span class="nf">diag</span><span class="p">(</span><span class="n">c22</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">denom</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">e_c11</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">e_c22</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">arcsin</span><span class="p">(</span><span class="n">c12</span> <span class="o">/</span> <span class="n">denom</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">jnp</span><span class="p">.</span><span class="n">pi</span><span class="p">)</span>
</code></pre></div></div> <p>and the generalization error being</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">order_parameter_loss</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">R</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="c1"># Student overlaps
</span>    <span class="n">I2_1</span> <span class="o">=</span> <span class="nf">get_I2</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">Q</span><span class="p">)</span>
    <span class="n">first_term</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">I2_1</span><span class="p">)</span>

    <span class="c1"># Student teacher overlaps
</span>    <span class="n">I2_2</span> <span class="o">=</span> <span class="nf">get_I2</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">second_term</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">I2_2</span><span class="p">)</span>

    <span class="c1"># Teacher overlaps
</span>    <span class="n">I2_3</span> <span class="o">=</span> <span class="nf">get_I2</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
    <span class="n">third_term</span> <span class="o">=</span> <span class="n">jnp</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">I2_3</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">first_term</span><span class="o">/</span><span class="mi">2</span> <span class="o">-</span> <span class="n">second_term</span> <span class="o">+</span> <span class="n">third_term</span><span class="o">/</span><span class="mi">2</span> <span class="o">+</span> <span class="n">sigma</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="mi">2</span>
</code></pre></div></div> <p>$I_{3}$, $I_{4}$ and $J_{2}$ follow a similar structure, with the covariance structure defined by the order parameters between the arguments of the expectation. These integrals appear in the order parameters dynamical equation by expanding the error signal $\Delta_{s}^{u}$, giving</p> \[\begin{align} \frac{\mathrm{d}R_{in}}{\mathrm{d}t} &amp; = \eta_{w} \left[ \sum_{m=1}^{M} I_{3}(i,n,m) - \sum_{j=1}^{K} I_{3}(i, n, j) \right] \\ \frac{\mathrm{d}Q_{ik}}{\mathrm{d}t} &amp; = \eta_{w} \left[ \sum_{m=1}^{M} I_{3}(i,k,m) - \sum_{j=1}^{K} I_{3}(i, k, j) \right] \\ &amp; + \eta_{w} \left[ \sum_{m=1}^{M} I_{3}(k, i, m) - \sum_{j=1}^{K} I_{3}(k, i, j) \right] \\ &amp; + \eta_{w}^{2} \left[ \sum_{m, n}^{M} I_{4}(i, k, n, m) - 2 \sum_{j, n} I_{4}(i, k, j, n) + \sum_{j, l} I_{4}(i, k, j, l) + \sigma^{2} J_{2}(i, j) \right]. \end{align}.\] <p>The closed form expression for all integrals $I_{3}$, $I_{4}$ and $J_{2}$ can be found in <d-cite key="saad1995online"></d-cite>.</p> <h3 id="solvable-learning-dynamics-for-neural-networks-with-a-hidden-layer">Solvable learning dynamics for neural networks with a hidden layer</h3> <p>From the equations above, extending to a two-layer network (<em>i.e.</em>, a network with a hidden layer) can be done simply by adding another layer to both the teacher and student networks. The second layer of the student network can be treated as an additional order parameter, as in Goldt et al. <d-cite key="goldt2020dynamics"></d-cite>. The expectations and integrals remain the same as in Saad &amp; Solla <d-cite key="saad1995online"></d-cite>, except that each update now involves the second layer of both the student and teacher networks.</p> \[\begin{align} \frac{\mathrm{d}R_{\text{in}}}{\mathrm{d}t} &amp;= \eta_w v_i \left[ \sum_{m=1}^M v_m^* I_{3}(i,n,m) - \sum_{j=1}^K v_j I_{3}(i,n,j) \right] \\ \frac{\mathrm{d}Q_{ik}}{\mathrm{d}t} &amp;= \eta_w v_i \left[ \sum_{m=1}^M v_m^* I_{3}(i,k,m) - \sum_{j=1}^K v_j I_{3}(i,k,j) \right] \\ &amp;+ \eta_w v_k \left[ \sum_{m=1}^M v_m^* I_{3}(i,k,m) - \sum_{j=1}^K v_j I_{3}(i,k,j) \right] \\ &amp;+ \eta_w^2 v_i v_k \left[ \sum_{n=1}^M \sum_{m=1}^M v_m^* v_n^* I_{4}(i,k,n,m) - 2 \sum_{j=1}^K \sum_{n=1}^M v_j v_n^* I_{4}(i,k,j,n) \right] \\ &amp;+ \eta_w^2 v_i v_k \left[ \sum_{j=1}^K \sum_{l=1}^K v_j v_l I_{4}(i,k,j,l) + \sigma^2 J_{2}(i,k) \right] \\ \frac{\mathrm{d}v_i}{\mathrm{d}t} &amp;= \eta_w \left[ \sum_{n=1}^M v_n^* I_{2}(i,n) - \sum_{j=1}^K v_j I_{2}(i,j) \right] \end{align}\] <p>where we introduce the second layer of the teacher and student as $v^{*}$ and $v$, respectively. The generalization error equation is also modified to include the second layer:</p> \[\begin{align} \mathcal{E} = \frac{1}{2}\sum_{i,k}v_{i}v_{k}I_{2}(i,k) + \frac{1}{2}\sum_{n,m}v_{n}^{*}v_{m}^{*}I_{2}(n,m) - \sum_{i, n}v_{i}v_{n}^{*}I_{2}(i,n)~. \end{align}\] <p>The derivation of these equations follow in the same way as the soft-committee machine with the only difference being the inclusion of the second-layer weights. Another way to understand the connection between both frameworks is by considering, for example, the soft committee machine as a two-layer network, where the second layer is fixed with ones in all its entries, <em>i.e.</em>, \(v^{*}_{n}=1\) and \(v_{i}=1\) for all entries of each vector. At this point, the reader should be well-equipped to follow the derivation in <d-cite key="goldt2020dynamics"></d-cite> as an extension of the results presented here from <d-cite key="saad1995online"></d-cite>.</p> <h2 id="replications">Replications</h2> <p>The code to reproduce all plots in this blog post can be found at <a href="https://anonymous.4open.science/r/nndyn-E763">https://anonymous.4open.science/r/nndyn-E763</a>. The code is written as a Python package called <code class="language-plaintext highlighter-rouge">nndyn</code> and implemented in <code class="language-plaintext highlighter-rouge">JAX</code> to take advantage of vectorization and JIT-compilation. The code has three main components: <strong>tasks</strong>, <strong>networks</strong>, and <strong>ordinary differential equations (ODEs)</strong>.</p> <p>A task is defined by a teacher network, which is used to sample $(x, y)$ pairs for training the student network. A common workflow in the teacher-student setup involves first simulating a student network trained numerically with gradient descent. The resulting dynamics are then compared to the derived ODEs for the order parameters of the student network.</p> <p>To simulate numerical training of a student network, the following pseudo-code can be used to define the teacher and student networks:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">nndyn.tasks</span> <span class="kn">import</span> <span class="n">TeacherNetErrorFunc</span>
<span class="kn">from</span> <span class="n">nndyn.networks</span> <span class="kn">import</span> <span class="n">ErfTwoLayerNet</span><span class="p">,</span> <span class="n">SoftCommettee</span>

<span class="n">data</span> <span class="o">=</span> <span class="nc">TeacherNetErrorFunc</span><span class="p">(</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">W1</span><span class="o">=</span><span class="n">W1</span><span class="p">,</span> <span class="n">W2</span><span class="o">=</span><span class="n">W2</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">batch_sampling_seed</span><span class="p">,</span>
                           <span class="n">additive_output_noise</span><span class="o">=</span><span class="n">additive_output_noise</span><span class="p">)</span>  <span class="c1"># or sigma in the equations
</span>
<span class="k">if</span> <span class="n">saad_solla</span><span class="p">:</span>
    <span class="n">true_student</span> <span class="o">=</span> <span class="nc">SoftCommettee</span><span class="p">(</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                 <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                 <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                 <span class="n">weight_scale</span><span class="o">=</span><span class="n">weight_scale</span><span class="p">,</span>
                                 <span class="n">seed</span><span class="o">=</span><span class="n">true_student_init_seed</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">true_student</span> <span class="o">=</span> <span class="nc">ErfTwoLayerNet</span><span class="p">(</span><span class="n">layer_sizes</span><span class="o">=</span><span class="p">(</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
                                  <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                                  <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
                                  <span class="n">weight_scale</span><span class="o">=</span><span class="n">weight_scale</span><span class="p">,</span>
                                  <span class="n">seed</span><span class="o">=</span><span class="n">true_student_init_seed</span><span class="p">)</span>
</code></pre></div></div> <p>Then, the training loop can be defined as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W1_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">W2_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_list</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">sample_batch</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">W1_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">true_student</span><span class="p">.</span><span class="n">W1</span><span class="p">))</span>
        <span class="n">W2_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">true_student</span><span class="p">.</span><span class="n">W2</span><span class="p">))</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">true_student</span><span class="p">.</span><span class="nf">update</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">loss_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">loss</span><span class="p">))</span>
</code></pre></div></div> <p>For the ODE calculation, an <code class="language-plaintext highlighter-rouge">ode</code> object can be created, where each order parameter are simply attributes of this object, which can be moved forward in time using the update method:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">nndyn.odes</span> <span class="kn">import</span> <span class="n">StudentEq</span><span class="p">,</span> <span class="n">SoftCommetteeEq</span>

<span class="c1"># Initialize the student ODE object
</span><span class="k">if</span> <span class="n">saad_solla</span><span class="p">:</span>
    <span class="n">ode</span> <span class="o">=</span> <span class="nc">SoftCommetteeEq</span><span class="p">(</span><span class="n">init_W1</span><span class="o">=</span><span class="n">student_W1</span><span class="p">,</span>
                          <span class="n">init_W2</span><span class="o">=</span><span class="n">student_W2</span><span class="p">,</span>  <span class="c1"># These are defined as ones for the committee machine
</span>                          <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                          <span class="n">time_constant</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span>
                          <span class="n">teacher_W1</span><span class="o">=</span><span class="n">teacher_W1</span><span class="p">,</span>
                          <span class="n">teacher_W2</span><span class="o">=</span><span class="n">teacher_W2</span><span class="p">,</span>  <span class="c1"># These are defined as ones for the committee machine
</span>                          <span class="n">sigma</span><span class="o">=</span><span class="n">additive_output_noise</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">ode</span> <span class="o">=</span> <span class="nc">StudentEq</span><span class="p">(</span><span class="n">init_W1</span><span class="o">=</span><span class="n">student_W1</span><span class="p">,</span>
                    <span class="n">init_W2</span><span class="o">=</span><span class="n">student_W2</span><span class="p">,</span>
                    <span class="n">learning_rate</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">,</span>
                    <span class="n">time_constant</span><span class="o">=</span><span class="n">dt</span><span class="p">,</span>
                    <span class="n">teacher_W1</span><span class="o">=</span><span class="n">teacher_W1</span><span class="p">,</span>
                    <span class="n">teacher_W2</span><span class="o">=</span><span class="n">teacher_W2</span><span class="p">,</span>
                    <span class="n">sigma</span><span class="o">=</span><span class="n">additive_output_noise</span><span class="p">)</span>

<span class="n">order_parameter_loss</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_param_R</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_param_Q</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">order_parameter_W2</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">save_every</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">n_steps</span><span class="p">):</span>
    <span class="c1"># Move order parameters forward in time by one dt step
</span>    <span class="c1"># Every variable is on Jax, so it is useful to convert them to numpy arrays before saving
</span>    <span class="n">order_loss</span> <span class="o">=</span> <span class="n">ode</span><span class="p">.</span><span class="nf">update</span><span class="p">()</span>
    <span class="n">order_parameter_loss</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">order_loss</span><span class="p">))</span>
    <span class="k">if</span>  <span class="n">i</span> <span class="o">%</span> <span class="n">save_every</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">order_param_R</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">ode</span><span class="p">.</span><span class="n">R</span><span class="p">))</span>
        <span class="n">order_param_Q</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">ode</span><span class="p">.</span><span class="n">Q</span><span class="p">))</span>
        <span class="n">order_parameter_W2</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span><span class="n">ode</span><span class="p">.</span><span class="n">student_W2</span><span class="p">))</span>
</code></pre></div></div> <p>We now present results comparing simulations of the low-dimensional ODEs derived by Saad &amp; Solla <d-cite key="saad1995online"></d-cite> with numerical simulations of training the full student network with standard minibatch stochastic gradient descent.</p> <h3 id="theory-experiment-overlap-in-the-soft-committee-machine">Theory-experiment overlap in the soft committee machine</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_saad_solla-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_saad_solla-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_saad_solla-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_saad_solla.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 1: Replication of the Saad and Solla <d-cite key="saad1995online"></d-cite> results. Simulated training of a soft committee machine student network with a fixed teacher network (blue) is compared against the analytical ODEs describing the time evolution of the order parameters (red). In this setup, N = 784, M = 4, and K varies by column. Notably, the generalization error is significantly reduced when the student network has a size of K = 4 or larger, since the student is "realizable" (has sufficient parameters) with respect to the teacher. </div> <h3 id="theory-experiment-overlap-in-neural-networks-with-a-hidden-layer">Theory-experiment overlap in neural networks with a hidden layer</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_base_goldt-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_base_goldt-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_base_goldt-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/fixed_teacher_student_base_goldt.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 2: Simulated training of a two-layer nonlinear student network using the analytical extension <d-cite key="goldt2020dynamics"></d-cite> with a fixed teacher network, compared against the analytical ODEs for the order parameters. In this setup, N = 784, M = 4, and K varies. Notably, the generalization error is also significantly reduced when the student network has a size of K = 4 or larger, as in the soft committee machine case. The alignment for Q, R, and v corresponds to the dot product between the measured order parameter in the trained network compared to the theoretical one described by the ODEs. Note that all alignments are close to 1, indicating that the ODEs accurately describe the training dynamics. Small-magnitude drops can be seen in the alignment when the loss function is steepest, due to fluctuations in finite-width training near these phase transitions. </div> <h3 id="large-initial-weights-produce-individual-differences">Large initial weights produce individual differences</h3> <div class="row mt-3"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/varying_weights-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/varying_weights-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/varying_weights-1400.webp"/> <img src="/ai810/assets/img/2025-04-28-analytical-simulated-dynamics/varying_weights.png" class="img-fluid" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Figure 3: Simulated two layer nonlinear student network using <d-cite key="goldt2020dynamics"></d-cite> for different initial weights in the student networks. The ODEs describe the dynamics for different initial conditions, corresponding to unique initializations of the student networks. </div> <h2 id="discussion">Discussion</h2> <p>The analytical techniques pioneered by Saad &amp; Solla <d-cite key="saad1995online"></d-cite> and others have inspired two broad directions of research: extending the theoretical framework to handle more complex scenarios, and applying these tools to analyze specific phenomena in machine learning. We detail these directions, and conclude with a brief forward-looking perspective.</p> <h3 id="applications-of-the-teacher-student-setting">Applications of the teacher-student setting</h3> <p>Beyond characterizing generalization error, the teacher-student framework has been applied to a wide range of problems, often to model interesting phenomena in machine learning. In optimization, applications extend to learning algorithms including natural gradient descent <d-cite key="yang1998complexity"></d-cite>, feedback alignment <d-cite key="refinetti2022align"></d-cite>, multi-pass SGD <d-cite key="arnaboldi2024repetita"></d-cite>, and reinforcement learning <d-cite key="bordelon2023loss"></d-cite> <d-cite key="patel2023rl"></d-cite>. Simsek and Martinelli et al. used the framework to reduce overparameterized deep networks to a minimal size by exploiting student neurons with similar tuning patterns to teacher neurons <d-cite key="simsek2021geometry"></d-cite> <d-cite key="martinelli2023expand"></d-cite>.</p> <p>The teacher-student framework has been used extensively to study the effect of task properties on learning dynamics via specific teacher parameterizations. Arnaboldi et al. developed quantitative measures of task difficulty <d-cite key="arnaboldi2024online"></d-cite>. Many analyses examine catastrophic forgetting and continual learning <d-cite key="straat2018statistical"></d-cite> <d-cite key="lee2021continual"></d-cite> <d-cite key="asanuma2021statistical"></d-cite> <d-cite key="hiratani2024disentangling"></d-cite>, transfer learning <d-cite key="lee2022maslow"></d-cite> <d-cite key="tahir2024features"></d-cite>, and meta-learning <d-cite key="wang2024dynamics"></d-cite> with teacher-student. Even current work under review at the ICLR 2025 conference <d-cite key="anonymous2024analyzing"></d-cite> <d-cite key="anonymous2024optimal"></d-cite> <d-cite key="anonymous2024theory"></d-cite> applies the teacher-student framework to study additional settings.</p> <h3 id="the-analytical-frontier">The analytical frontier</h3> <p>The statistical physics approach to neural network dynamics has expanded significantly beyond the early results of Saad &amp; Solla <d-cite key="saad1995online"></d-cite> and others. Early extensions to teacher-student explored different activation functions, with Freeman and Saad analyzing radial basis function networks <d-cite key="freeman1997online"></d-cite>. Richert et al. studied the qualitative convergence for these dynamical systems <d-cite key="richert2022soft"></d-cite>. Deep networks were analyzed by Tian et al., who first provided empirical evidence for specialization in deep teacher-student networks <d-cite key="tian2019luck"></d-cite>, then developed theoretical characterization of these dynamics <d-cite key="tian2020student"></d-cite>.</p> <p>Recent work has tackled increasingly complex learning scenarios. Loureiro et al. <d-cite key="loureiro2021learning"></d-cite> and Arnaboldi et al. <d-cite key="arnaboldi2023highdimensional"></d-cite> extended the framework to new learning settings, while Bardone et al. analyzed systems with correlated latent variables <d-cite key="bardone2024sliding"></d-cite>. Questions of learnability have been addressed by Troiani et al. <d-cite key="troiani2024fundamental"></d-cite>, who established theoretical limits on what neural networks can learn in various settings.</p> <p>The Gaussian equivalence property <d-cite key="goldt2020modelling"></d-cite> <d-cite key="goldt2021gaussian"></d-cite> demonstrated that many results derived for Gaussian inputs extend to other data distributions, broadening the applicability of these analytical techniques. However, it is still challenging to capture the effect on learning dynamics of strongly non-Gaussian input distributions, and this frontier is attracting significant interest <d-cite key="ingrosso2022datadriven"></d-cite> <d-cite key="refinetti2023neural"></d-cite>.</p> <h3 id="conclusions">Conclusions</h3> <p>The mathematical tools of statistical physics have proven an important component to the development of neural networks, as noted by this year’s Nobel Prize in Physics <d-cite key="zotero-4179"></d-cite>. The teacher-student framework we explored here represents one successful application of physics-inspired analysis to the analysis of neural network dynamics. By reducing complex learning dynamics to tractable macroscopic variables, this approach provides exact solutions that characterize how neural networks learn and generalize.</p> <p>While the analytical teacher-student settings are simplified compared to modern deep learning systems, they nevertheless captures fundamental aspects of learning dynamics that persist in more complex architectures, including feature learning as characterized by the specialization transition. The extensions and applications surveyed here show how these theoretical tools continue to provide insights into problems ranging from optimization to continual learning. We hope that this blog post and the accompanying code repository make these results more accessible and extensible to the broader machine learning community.</p> <hr/> <h4 id="code-availability">Code availability</h4> <p>The code to reproduce all plots in this blog post can be found at <a href="https://anonymous.4open.science/r/nndyn-E763">https://anonymous.4open.science/r/nndyn-E763</a>. This codebase is also easily adaptable to explore the learning dynamics of neural networks in the teacher-student setting beyond the scope of this blog post.</p>]]></content><author><name>Rodrigo Carrasco-Davis</name></author><summary type="html"><![CDATA[The learning dynamics of neural networks—in particular, how parameters change over time during training—describe how data, architecture, and algorithm interact in time to produce a trained neural network model. Characterizing these dynamics, in general, remains an open problem in machine learning, but, handily, restricting the setting allows careful empirical studies and even analytical results. In this blog post, we review approaches to analyzing the learning dynamics of nonlinear neural networks, focusing on a particular setting known as teacher-student that permits an explicit analytical expression for the generalization error of a nonlinear neural network trained with online gradient descent. We provide an accessible mathematical formulation of this analysis and a JAX codebase to implement simulation of the analytical system of ordinary differential equations alongside neural network training in this setting. We conclude with a discussion of how this analytical paradigm has been used to investigate generalization in neural networks and beyond.]]></summary></entry></feed>